{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9177b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy import zeros,ones,expand_dims,hstack\n",
    "from numpy.random import randn,randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Dense,Reshape,Flatten,Conv2D,Conv2DTranspose,LeakyReLU,BatchNormalization,Activation\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac7a9063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator_and_recognition(cat_dim, in_shape=(32,32,1)):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    cat_dim:  number of categorical variables present in the dataset for our study\n",
    "    in_shape: input dimension vectors for our datatset\n",
    "    \"\"\"\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # image input\n",
    "    in_image = Input(shape=in_shape)\n",
    "    # layer 1\n",
    "    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "    d = LeakyReLU(alpha=0.1)(d)\n",
    "    # layer 2\n",
    "    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = LeakyReLU(alpha=0.1)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    # layer 3\n",
    "    d = Conv2D(256, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "    d = LeakyReLU(alpha=0.1)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    # flatten feature maps\n",
    "    d = Flatten()(d)\n",
    "    # real/fake output\n",
    "    out_classifier = Dense(1, activation='sigmoid')(d)\n",
    "    # define d model\n",
    "    d_model = Model(in_image, out_classifier)\n",
    "    # compile d model\n",
    "    d_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
    "    \n",
    "    \n",
    "    # create q model layers\n",
    "    q = Dense(128)(d)\n",
    "    q = BatchNormalization()(q)\n",
    "    q = LeakyReLU(alpha=0.1)(q)\n",
    "    # q model output\n",
    "    out_codes = Dense(cat_dim, activation='softmax')(q)\n",
    "    # define q model\n",
    "    q_model = Model(in_image, out_codes)\n",
    "    return d_model, q_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8842c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(generator_input_dim):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    \n",
    "    # image generator input\n",
    "    input_dim = Input(shape=(generator_input_dim,))\n",
    "    \n",
    "    # FC -layer 1\n",
    "    gen = Dense(2*2*448, kernel_initializer=init)(input_dim)\n",
    "    gen = Activation('relu')(gen)\n",
    "    gen = BatchNormalization()(gen)\n",
    "    gen = Reshape((2, 2, 448))(gen)\n",
    "    \n",
    "    # Deconv -layer 2\n",
    "    gen = Conv2DTranspose(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "    gen = Activation('relu')(gen)\n",
    "    gen = BatchNormalization()(gen)\n",
    "    \n",
    "    # Deconv -layer 3\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "    gen = Activation('relu')(gen)\n",
    "  \n",
    "    # Deconv -layer 4\n",
    "    gen = Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "    gen = Activation('relu')(gen)\n",
    "    \n",
    "    # Deconv -layer 5\n",
    "    gen = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "    # tanh output\n",
    "    out_layer = Activation('tanh')(gen)\n",
    "    \n",
    "    # define model\n",
    "    gen_model = Model(input_dim, out_layer)\n",
    "    \n",
    "    return gen_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "283aae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model, q_model):\n",
    "    # make weights in the discriminator (some shared with the q model) as not trainable\n",
    "    for layer in d_model.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False\n",
    "    # connect g outputs to d inputs\n",
    "    d_output = d_model(g_model.output)\n",
    "    # connect g outputs to q inputs\n",
    "    q_output = q_model(g_model.output)\n",
    "    # define composite model\n",
    "    model = Model(g_model.input, [d_output, q_output])\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=['binary_crossentropy', 'categorical_crossentropy'], optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c4583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
